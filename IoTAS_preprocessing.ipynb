{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IoTAS_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOd0fixp3cNN1Meg7Ar2kI1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MCasari-PMEL/IoTAS/blob/master/IoTAS_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p5v0Jum64vB",
        "colab_type": "text"
      },
      "source": [
        "# IoTAS Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0J1fVCR69p4",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "This notebook performs the preprocessing on the raw IoTAS Short-burst data (SBD) messages that are pulled out of an inbox into a google drive.\n",
        "\n",
        "## Order of operation\n",
        "1. Authenticate with Google Drive\n",
        "1. Convert Raw Files\n",
        "  1. Loop through Raw Data Folders\n",
        "  1. Loop through Raw Data Folder File\n",
        "  1. Copy file into temporary Colab workspace file\n",
        "  1. Run file contents through IoTAS byte parser\n",
        "  1. Append data to appropriate system CSV file\n",
        "  1. Copy temporary file to archived data folder\n",
        "  1. Delete temporary file\n",
        "  1. Remove original file\n",
        "1. Create XXX_processed.csv file with all historic data\n",
        "  1. Loop through CSV Folders\n",
        "  1. Import Old _processed.csv file into pandas dataframe\n",
        "  1. Import New CSV Files into Pandas Dataframe\n",
        "  1. Append New dataframe to old dataframe\n",
        "  1. Sort Dataframe (oldest to newest)\n",
        "  1. Remove duplicate entries\n",
        "  1. Save dataframe (overwrite previous _processed.csv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G71ppqTy6zKS",
        "colab_type": "text"
      },
      "source": [
        "## Authentication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WspN4Vug3Dxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Packages needed for accessing google drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4Dt_d2n8QiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticating with your user account\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkKlNHIj8aE_",
        "colab_type": "text"
      },
      "source": [
        "## Path Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBh6HpxrrXSc",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@markdown ---\n",
        "#@markdown ## Do not change this unless necessary\n",
        "#@markdown ### Enter a Path  to Top Data Folder:\n",
        "data_path = \"Test/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Enter a Path to Archived Data Folder:\n",
        "archive_path = \"Test/Archive\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Enter the Path to Converted Data CSV:\n",
        "save_path = \"Test/Converted\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl1XQWx-9END",
        "colab_type": "text"
      },
      "source": [
        "## Packages to include"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YRgZzD-soS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "55e16b54-549f-44dc-847f-0285710775ac"
      },
      "source": [
        "!git clone https://gist.github.com/dc7e60aa487430ea704a8cb3f2c5d6a6.git /tmp/colab_util_repo\n",
        "!mv /tmp/colab_util_repo/colab_util.py colab_util.py \n",
        "!rm -r /tmp/colab_util_repo"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/tmp/colab_util_repo'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Total 40 (delta 0), reused 0 (delta 0), pack-reused 40\u001b[K\n",
            "Unpacking objects:   2% (1/40)   \rUnpacking objects:   5% (2/40)   \rUnpacking objects:   7% (3/40)   \rUnpacking objects:  10% (4/40)   \rUnpacking objects:  12% (5/40)   \rUnpacking objects:  15% (6/40)   \rUnpacking objects:  17% (7/40)   \rUnpacking objects:  20% (8/40)   \rUnpacking objects:  22% (9/40)   \rUnpacking objects:  25% (10/40)   \rUnpacking objects:  27% (11/40)   \rUnpacking objects:  30% (12/40)   \rUnpacking objects:  32% (13/40)   \rUnpacking objects:  35% (14/40)   \rUnpacking objects:  37% (15/40)   \rUnpacking objects:  40% (16/40)   \rUnpacking objects:  42% (17/40)   \rUnpacking objects:  45% (18/40)   \rUnpacking objects:  47% (19/40)   \rUnpacking objects:  50% (20/40)   \rUnpacking objects:  52% (21/40)   \rUnpacking objects:  55% (22/40)   \rUnpacking objects:  57% (23/40)   \rUnpacking objects:  60% (24/40)   \rUnpacking objects:  62% (25/40)   \rUnpacking objects:  65% (26/40)   \rUnpacking objects:  67% (27/40)   \rUnpacking objects:  70% (28/40)   \rUnpacking objects:  72% (29/40)   \rUnpacking objects:  75% (30/40)   \rUnpacking objects:  77% (31/40)   \rUnpacking objects:  80% (32/40)   \rUnpacking objects:  82% (33/40)   \rUnpacking objects:  85% (34/40)   \rUnpacking objects:  87% (35/40)   \rUnpacking objects:  90% (36/40)   \rUnpacking objects:  92% (37/40)   \rUnpacking objects:  95% (38/40)   \rUnpacking objects:  97% (39/40)   \rUnpacking objects: 100% (40/40)   \rUnpacking objects: 100% (40/40), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpSZWpYC82jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from colab_util import *\n",
        "drive_handler = GoogleDriveHandler()\n",
        "import os\n",
        "import math\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF6EqFsQu9Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the IoTAS parse file here\n",
        "drive_handler.download('iotas_parser.py', target_path=\"Colab Scripts/iotas_parser.py\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux8MuX3evBi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import iotas_parser as iotas"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lRv8VMLyULV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade --quiet gspread\n",
        "import gspread\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d97rYRjlPTFU",
        "colab_type": "text"
      },
      "source": [
        "## Raw Data Processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg6WnYGgtS9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path_id = drive_handler.path_to_id(data_path)\n",
        "archive_path_id = drive_handler.path_to_id(archive_path)\n",
        "save_path_id = drive_handler.path_to_id(save_path)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCgE-DN9mzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = drive_handler.list_folder(root_folder_id=data_path_id)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om8cO0Cf-egK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7fc979a1-3524-47c3-96b2-cbd8293d2051"
      },
      "source": [
        "for folder in data_folder:\n",
        "  if ((folder['title'].lower().find('archive') < 0) and \n",
        "        (folder['title'].lower().find('converted') < 0)):\n",
        "    files = drive_handler.list_folder(root_folder_id=folder['id'])\n",
        "\n",
        "    save_folder = drive_handler.create_folder(folder['title'], parent_path=save_path)\n",
        "\n",
        "    for idx, file in enumerate(files):\n",
        "      # Upload and copy file to Archive \n",
        "      file_name = drive_handler.list_folder(root_folder_id=folder['id'])[idx]['title']\n",
        "      file_path = data_path + folder['title'] + '/' + file_name      \n",
        "      drive_handler.download(file_name, target_path=file_path)\n",
        "      drive_handler.upload(file_name, parent_path=archive_path, overwrite=False)\n",
        "  \n",
        "      # Open the file and read and parse contents\n",
        "      with open(file_name,'rb') as f:\n",
        "        (i_date, contents) = iotas.read_file(f)\n",
        "\n",
        "      # Create CSV in Data Folder\n",
        "      save_file_name = file_name.replace('sbd','csv')\n",
        "      uploaded = drive.CreateFile({\n",
        "          'title':save_file_name,\n",
        "          'parents': [{'kind': 'drive#fileLink', 'id': save_folder}]})\n",
        "      uploaded.SetContentString(contents)\n",
        "      uploaded.Upload()\n",
        "\n",
        "      # Delete temporary file from Colab workspace\n",
        "      !rm $file_name\n",
        "\n",
        "    \n",
        "    # Delete files already transfered\n",
        "    if len(files) > 0:\n",
        "      print(f\"Delete files in folder: {folder['title']}\\n\")\n",
        "      temp_str = f\"\\'{folder['id']}\\' in parents\"\n",
        "      file_list = drive.ListFile({'q': temp_str}).GetList()\n",
        "\n",
        "      for f in file_list:\n",
        "        print(f\"Deleting file {f['title']} from data\")\n",
        "        f.Delete()\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300434064218040 already exists\n",
            "300434064219040 already exists\n",
            "300434064215020 already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIxHYCTjQdsp",
        "colab_type": "text"
      },
      "source": [
        "## CSV Data Procesing\n",
        "Convert all of the CSV files in the directory into a Pandas dataframe, and ultimately a processed CSV file for easy data access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUPXJoH7Rh8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "columns = {'Date (yyyy-mm-dd hh:mm:ss)', 'latitude', 'longitude', 'pressure (mbar)', 'latitude offset', 'longitude offset'}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ouQNGn4I_EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_folder = drive_handler.list_folder(root_folder_id=save_path_id)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvOnrOAxQlee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3dcf02fe-16ff-47e8-9118-5615fe18bcf2"
      },
      "source": [
        "for folder in csv_folder:\n",
        "  # Load or Create Processed CSV Folder\n",
        "  processed_name = folder['title'] + \"_processed.csv\"\n",
        "  processed_path = save_path + '/' + folder['title'] + '/' \n",
        "  save_folder_id = drive_handler.path_to_id(processed_path)\n",
        "  processed_file_path = processed_path + processed_name\n",
        "  file_exists = True\n",
        "  try:\n",
        "    drive_handler.download(processed_name, target_path=processed_file_path )\n",
        "  except:\n",
        "    file_exists = False\n",
        "\n",
        "  # Get the files in the current folder\n",
        "  files = drive_handler.list_folder(root_folder_id=folder['id'])\n",
        "  file_names = [x['title'] for x in files]\n",
        "  \n",
        "  # Create the dataframe\n",
        "  data = pd.DataFrame(columns=columns)\n",
        "  if file_exists:\n",
        "    try:\n",
        "      # Injest the old data, if it exists already\n",
        "      data = pd.read_csv(processed_name,delimiter=',',header=0)\n",
        "      files.pop(file_names.index(processed_name))\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  # Create an archive folder\n",
        "  save_folder = drive_handler.create_folder('Archive', parent_path=processed_path)\n",
        "  \n",
        "  # Remove Archive from list\n",
        "  try:\n",
        "    files.pop(file_names.index('Archive'))\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  # Injest old data\n",
        "  for f in files:\n",
        "    file_name = f['title']\n",
        "    if file_name != 'Archive':\n",
        "      downloaded = drive.CreateFile({'id':f['id']}) \n",
        "      downloaded.GetContentFile(file_name)  \n",
        "      try:\n",
        "        df = pd.read_csv(file_name, header=7)\n",
        "        data = pd.concat([data, df]).reset_index(drop=True)\n",
        "      except Exception as e:\n",
        "        print(f\"Failure to injest {file_name}\")\n",
        "        print(e)\n",
        "\n",
        "      # Save original file to Archive\n",
        "      uploaded = drive.CreateFile({\n",
        "          'title':file_name,\n",
        "          'parents': [{'kind': 'drive#fileLink', 'id': save_folder}]})\n",
        "      uploaded.SetContentFile(file_name)\n",
        "      uploaded.Upload()\n",
        "\n",
        "      # Delete the local instance\n",
        "      !rm $file_name\n",
        "  \n",
        "  # Delete the original file\n",
        "  temp_str = f\"\\'{folder['id']}\\' in parents\"\n",
        "  file_list = drive.ListFile({'q': temp_str}).GetList()\n",
        "\n",
        "  for f in file_list:\n",
        "    if f['title'] != 'Archive' and f['title'] != processed_name:\n",
        "      print(f\"Deleting file {f['title']} from data\")\n",
        "      f.Delete()\n",
        "\n",
        "  # Drop duplicate data\n",
        "  data = data.drop_duplicates(subset=['Date (yyyy-mm-dd hh:mm:ss)'], keep='last')\n",
        "\n",
        "  # Sort the data by date\n",
        "  data = data.sort_values(['Date (yyyy-mm-dd hh:mm:ss)'])\n",
        "  \n",
        "  # Save the dataframe as a csv\n",
        "  data.to_csv(processed_name)\n",
        "\n",
        "  # Move CSV to Google Drive (drive_handler.upload fails with overwrite,do it manually)\n",
        "  temp_str = f\"\\'{folder['id']}\\' in parents\"\n",
        "  file_list = drive.ListFile({'q': temp_str}).GetList()\n",
        "  for file in file_list:\n",
        "    if file['title'] == processed_name:\n",
        "      file.Delete()\n",
        "  file = drive.CreateFile(\n",
        "      {\n",
        "          'title': processed_name, \n",
        "          'parents': [{'kind': 'drive#fileLink', 'id': save_folder_id}]\n",
        "\n",
        "      }\n",
        "  )\n",
        "  file.SetContentFile(processed_name)\n",
        "  file.Upload()\n",
        "\n",
        "  # Delete the local processed file \n",
        "  !rm $processed_name"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive already exists\n",
            "Archive already exists\n",
            "Archive already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX2D1yHGcWWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}